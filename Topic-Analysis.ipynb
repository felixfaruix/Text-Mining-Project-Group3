{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb683c59",
   "metadata": {},
   "source": [
    "# Topic Analysis - Project Assignment Group 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, get_dataset_config_names\n",
    "import pyLDAvis\n",
    "import pandas as pd \n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pycountry\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle \n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS \n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9289e0",
   "metadata": {},
   "source": [
    "### Dataset Inspection and Content Extraction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c987e2",
   "metadata": {},
   "source": [
    "In the first part of our Topic Analysis section of the project, we will extract all the sections in the database and count them to see what each news article is assigned to. In this way, we can choose our topics and see what we are first counting.\n",
    "\n",
    "We will notice that the _section_ column in the dataset is not helpful, since it groups the articles in a sparse way and does not provide useful labels for our topic classification.\n",
    "\n",
    "Thus, we will have to preprocess each article by first extracting the _content_ column from each (ignoring all the others) In this way we will feed our LDA model for topic analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolders = get_dataset_config_names(\"RealTimeData/bbc_news_alltime\")\n",
    "\n",
    "all_sections = []\n",
    "\n",
    "for month in subfolders:\n",
    "    dataset = load_dataset(\"RealTimeData/bbc_news_alltime\", month, split=\"train\")\n",
    "    if \"section\" in dataset.column_names:\n",
    "        all_sections.extend(dataset[\"section\"])\n",
    "\n",
    "section_df = pd.DataFrame(all_sections, columns=[\"section\"])\n",
    "section_counts = section_df[\"section\"].value_counts(dropna=False).reset_index()\n",
    "section_counts.columns = [\"Topic\", \"Count\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c3b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_counts = section_df['section'].value_counts(dropna=False).reset_index()\n",
    "section_counts.columns = ['Topic', 'Count']\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(section_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841ac7d",
   "metadata": {},
   "source": [
    "Loading SpaCy, getting the dataset and splitting between test and train folders (70/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe29be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolders = get_dataset_config_names(\"RealTimeData/bbc_news_alltime\")\n",
    "preprocessing = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "preprocessing.add_pipe(\"sentencizer\")\n",
    "random.seed(42)\n",
    "selected_months = random.sample(subfolders, k=20)\n",
    "\n",
    "#Splitting the folders in train and test sets based on months folders \n",
    "train_months, test_months = train_test_split(selected_months, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0014f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = []\n",
    "\n",
    "# Looping over the months folders\n",
    "for month in tqdm(train_months, desc=\"Month\"): \n",
    "    dataset = load_dataset(\"RealTimeData/bbc_news_alltime\", month, split=\"train\")\n",
    "    # Looping over the various articles\n",
    "    content = [article[\"content\"] for article in dataset] \n",
    "    # Getting the Doc object of the article and looping over them splittin the sentences\n",
    "    for doc in preprocessing.pipe(content, batch_size=32):\n",
    "        for sent in doc.sents:\n",
    "            text = sent.text.strip()\n",
    "            if text:\n",
    "                print(text)\n",
    "                train_sentences.append(text)\n",
    "\n",
    "print(train_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682644b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theresa May was in Poland to sign a defence treaty with the country\n",
      "\n",
      "Theresa May has sought to reassure Polish people living in the UK that they are still welcome after Brexit.\n",
      "Speaking on a trip to Warsaw to sign a new defence treaty with the country, the PM said the one million Polish residents were a \"strong part of [UK] society\".\n",
      "She promised a \"simple\" and \"easy\" process to get \"settled status\" to remain after the UK leaves the EU.\n",
      "The trip comes after Mrs May sacked one of her closest allies, Damian Green.\n",
      "She asked him to leave after he made \"misleading\" statements about claims pornography was found on his parliamentary computer.\n"
     ]
    }
   ],
   "source": [
    "for x in train_sentences[:5]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0805123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273a167b60814c8fba2e419746141f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Months Analyzed:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we are applying the same procedure for the test set in the test_months folder \n",
    "test_sentences = []\n",
    "\n",
    "for month in tqdm(test_months, desc=\"Months Analyzed\"): \n",
    "    dataset = load_dataset(\"RealTimeData/bbc_news_alltime\", month, split=\"train\")\n",
    "    # Looping over the various articles\n",
    "    content = [article[\"content\"] for article in dataset] \n",
    "    # Getting the Doc object of the article and looping over them splittin the sentences\n",
    "    for doc in preprocessing.pipe(content, batch_size=32):\n",
    "        for sent in doc.sents:\n",
    "            text = sent.text.strip()\n",
    "            if text:\n",
    "                test_sentences.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc18c1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amiram Cooper, Guy Gilboa-Dalal and Tsachi Idan are being held in Gaza\n",
      "\n",
      "Israel says 129 people remain unaccounted for after they were abducted and taken to Gaza during the October 7 attacks by Hamas.\n",
      "Of these, Israel says that 22 are believed to be dead.\n",
      "A group representing the families of hostages says that Gadi Haggai, 73, is now believed to have died in Gaza.\n",
      "An estimated 240 people were taken prisoner, but 105 were later released by Hamas during a six-day ceasefire at the end of November.\n",
      "These are the stories of those hostages who are still being held, which have either been confirmed by the BBC or credibly reported.\n"
     ]
    }
   ],
   "source": [
    "for x in test_sentences[:5]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f987783",
   "metadata": {},
   "source": [
    "### Data Preprocessing using SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72950a4c",
   "metadata": {},
   "source": [
    "In this **preprocessing** stage, we transform raw sentences into clean, *lemmatized* tokens optimized for topic modeling. Specifically, we use **SpaCy** to tokenize and lemmatize each sentence, removing stopwords, punctuation, short words, and numbers. \n",
    "\n",
    "We also exclude tokens based on their linguistic roles (pronouns, determiners, prepositions, auxiliaries) and named entities (people, organizations, locations, dates, times). Then, we filter out domain-specific noise such as media sources (\"BBC\", \"Reuters\"), common journalistic fillers (\"said\", \"today\"), possessive pronouns, and country names using a custom exclusion list. \n",
    "\n",
    "The resulting cleaned tokens are then structured into a **Gensim-compatible dictionary and corpus**, ready for training the LDA model. \n",
    "This also takes into account the labels in the test set provided for the project. In fact, our goal is to align as closely as possible to the topics shown in that test set. Thus, we cut out all the noise we think we may encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a2dc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pos_ner = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n",
    "stopwords = token_pos_ner.Defaults.stop_words\n",
    "\n",
    "countries_list= {country.name.lower() for country in pycountry.countries}\n",
    "\n",
    "# Excluding some words that are indeed part of newspapers articles but not necessary\n",
    "custom_stop = {\n",
    "    \"bbc\",\"reuters\",\"cnn\",\"guardian\",\"nytimes\",\"telegraph\",\"aljazeera\",\n",
    "    \"news\",\"press\",\"article\",\"media\",\"coverage\",\"broadcast\",\"report\",\"headline\",\n",
    "    \"says\",\"said\",\"told\",\"claim\",\"claimed\",\"statement\",\n",
    "    \"thing\",\"stuff\",\"someone\",\"anyone\",\"everyone\",\"something\",\"everything\",\n",
    "    \"kind\",\"sort\",\"part\",\"place\",\"area\",\"around\"\n",
    "}\n",
    "\n",
    "# Unifying the various exclusions we listed above (costumized)\n",
    "all_exclusions = stopwords.union(custom_stop).union(countries_list)\n",
    "pos_exclusions  = {\"PRON\",\"DET\",\"ADP\",\"AUX\",\"INTJ\"}\n",
    "entities_excl   = {\"PERSON\",\"ORG\",\"GPE\",\"LOC\",\"DATE\",\"TIME\"}\n",
    "\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    \"\"\"\n",
    "    This function preprocesses sentences coming from test/train lists. \n",
    "    It removes the PoS tags adn entity labels listed above. It also removes unecessary words \n",
    "    that are not helpful for our topic analysis\n",
    "    \"\"\"\n",
    "\n",
    "    token_lists, cleaned_strings = [], []\n",
    "\n",
    "    for i in tqdm(range(0, len(train_sentences), 500), desc=\"Sentence batches\"):\n",
    "\n",
    "        batch_sents = train_sentences[i : i + 500]\n",
    "\n",
    "        for article in token_pos_ner.pipe(batch_sents, batch_size=16): \n",
    "            tokens = [token.lemma_.lower() for token in article if token.lemma_.lower() not in all_exclusions\n",
    "                and len(token) > 3\n",
    "                and not token.is_punct\n",
    "                and not token.like_num\n",
    "                and token.pos_ not in pos_exclusions\n",
    "                and token.ent_type_ not in entities_excl]\n",
    "            token_lists.append(tokens)\n",
    "            cleaned_strings.append(\" \".join(tokens))\n",
    "\n",
    "    return token_lists, cleaned_strings\n",
    "\n",
    "def preprocessing_test(sentence):\n",
    "    \"\"\"\n",
    "    This function preprocesses sentences coming from test/train lists. \n",
    "    It removes the PoS tags adn entity labels listed above. It also removes unecessary words \n",
    "    that are not helpful for our topic analysis\n",
    "    \"\"\"\n",
    "\n",
    "    token_lists, cleaned_strings = [], []\n",
    "\n",
    "    for i in tqdm(range(0, len(test_sentences), 500), desc=\"Sentence batches\"):\n",
    "\n",
    "        batch_sents = test_sentences[i : i + 500]\n",
    "\n",
    "        for article in token_pos_ner.pipe(batch_sents, batch_size=16): \n",
    "            tokens = [token.lemma_.lower() for token in article if token.lemma_.lower() not in all_exclusions\n",
    "                and len(token) > 3\n",
    "                and not token.is_punct\n",
    "                and not token.like_num\n",
    "                and token.pos_ not in pos_exclusions\n",
    "                and token.ent_type_ not in entities_excl]\n",
    "            token_lists.append(tokens)\n",
    "            cleaned_strings.append(\" \".join(tokens))\n",
    "\n",
    "    return token_lists, cleaned_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304aec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tokens_string = preprocessing(train_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4a1e8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed69877cfcb413d86f97188ed00b3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentence batches:   0%|          | 0/443 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_tokens, test_tokens_string = preprocessing_test(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce6d94",
   "metadata": {},
   "source": [
    "### Checkpoint \n",
    "\n",
    "(The Variable is too heavy to reload the process every time, so we saved the variables in a pkl file we can use it again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the variables for later usage, in order to not loose the preprocessing \n",
    "with open(\"train_processed.pkl\", \"wb\") as f: \n",
    "    pickle.dump({\"tokens\": train_tokens, \n",
    "                 \"strings\": train_tokens_string}, f) \n",
    "    \n",
    "# Saving the variables for later usage, in order to not loose the preprocessing \n",
    "with open(\"test_processed.pkl\", \"wb\") as f: \n",
    "    pickle.dump({\"tokens\": test_tokens, \n",
    "                 \"strings\": test_tokens_string}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d2ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_processed.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "train_tokens = data[\"tokens\"]\n",
    "train_token_strings = data[\"strings\"]\n",
    "\n",
    "with open(\"test_processed.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "train_tokens = data[\"tokens\"]\n",
    "train_token_strings = data[\"strings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d338caeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['theresa', 'sign', 'defence', 'treaty', 'country', 'theresa', 'seek', 'reassure', 'polish', 'people', 'live', 'welcome']\n",
      "['speak', 'trip', 'sign', 'defence', 'treaty', 'country', 'polish', 'resident', 'strong', 'society']\n",
      "['promise', 'simple', 'easy', 'process', 'settle', 'status', 'remain', 'leave']\n",
      "['trip', 'come', 'sack', 'close', 'ally']\n",
      "['ask', 'leave', 'misleading', 'pornography', 'find', 'parliamentary', 'computer']\n"
     ]
    }
   ],
   "source": [
    "for sentence in (train_tokens[:5]): \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ca1fd",
   "metadata": {},
   "source": [
    "### Training LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49746ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 08:45:50,887 : INFO : -8.561 per-word bound, 377.6 perplexity estimate based on a held-out corpus of 2000 documents with 14985 words\n",
      "2025-05-31 08:45:50,888 : INFO : PROGRESS: pass 3, at document #532000/625775\n",
      "2025-05-31 08:45:51,106 : INFO : merging changes from 2000 documents into a model of 625775 documents\n",
      "2025-05-31 08:45:51,106 : INFO : topic #9 (0.100): 0.025*\"right\" + 0.014*\"court\" + 0.012*\"display\" + 0.012*\"death\" + 0.011*\"post\" + 0.011*\"strike\" + 0.011*\"shot\" + 0.011*\"president\" + 0.010*\"chief\" + 0.010*\"attempt\"\n",
      "2025-05-31 08:45:51,123 : INFO : topic #4 (0.100): 0.097*\"play\" + 0.083*\"need\" + 0.073*\"video\" + 0.062*\"want\" + 0.060*\"browser\" + 0.050*\"use\" + 0.050*\"policy\" + 0.047*\"enable\" + 0.031*\"external\" + 0.031*\"cookie\"\n",
      "2025-05-31 08:45:51,125 : INFO : topic #0 (0.100): 0.076*\"content\" + 0.051*\"site\" + 0.047*\"responsible\" + 0.045*\"continue\" + 0.044*\"external\" + 0.041*\"view\" + 0.024*\"accept\" + 0.022*\"miss\" + 0.022*\"choose\" + 0.019*\"original\"\n",
      "2025-05-31 08:45:51,126 : INFO : topic #2 (0.100): 0.028*\"government\" + 0.025*\"minister\" + 0.015*\"party\" + 0.013*\"prime\" + 0.013*\"leader\" + 0.010*\"vote\" + 0.010*\"secretary\" + 0.010*\"rate\" + 0.009*\"decision\" + 0.009*\"plan\"\n",
      "2025-05-31 08:45:51,127 : INFO : topic #5 (0.100): 0.031*\"people\" + 0.019*\"work\" + 0.018*\"family\" + 0.016*\"service\" + 0.016*\"child\" + 0.015*\"life\" + 0.013*\"live\" + 0.012*\"help\" + 0.011*\"school\" + 0.010*\"tell\"\n",
      "2025-05-31 08:45:51,129 : INFO : topic diff=0.016173, rho=0.056176\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(train_tokens)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "bow_dict = [dictionary.doc2bow(doc) for doc in train_tokens]\n",
    "\n",
    "lda_model = models.LdaModel(corpus = bow_dict, id2word = dictionary, num_topics = 10, passes = 10, random_state = 42, eval_every = 1, per_word_topics   = True)\n",
    "\n",
    "for tid, topic in lda_model.print_topics(num_words=10):\n",
    "    print(f\"Topic {tid}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12666f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8066e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 1.  Pretty-print top words per topic\n",
    "# --------------------------------------------------\n",
    "def show_top_words(model, dictionary, n_words=10):\n",
    "    print(\"TOP WORDS PER TOPIC\\n\" + \"-\"*30)\n",
    "    for tid, topic in model.show_topics(num_topics=model.num_topics,\n",
    "                                        num_words=n_words,\n",
    "                                        formatted=False):\n",
    "        words = \", \".join([w for w, p in topic])\n",
    "        print(f\"Topic {tid:>2} ▶ {words}\")\n",
    "\n",
    "show_top_words(lda_model, dictionary, n_words=12)\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, bow_corpus, dictionary)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_VU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
