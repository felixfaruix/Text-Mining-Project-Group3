{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1fd426-08de-4b76-b54b-73fab7cc8e03",
   "metadata": {},
   "source": [
    "## Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e4199e-e1b5-49f7-a715-f6080aef1faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: seqeval in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: ipywidgets in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (8.1.7)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from datasets) (0.32.3)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from ipywidgets) (9.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets transformers seqeval ipywidgets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ea4ef-e0cb-4753-a5a2-ac0689dc9d64",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b3810b-5a9a-4efb-aebb-68028a6176cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178668a451434851bc80ca0388c386a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/22.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289888ed63a84b1b9457028fe33adc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conll2012_ontonotesv5.py:   0%|          | 0.00/32.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for conll2012_ontonotesv5 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2012_ontonotesv5.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y+\n",
      "The repository for conll2012_ontonotesv5 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2012_ontonotesv5.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7922fc31fad4553ba70bafabfb84ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/194M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2597ccbdfd144829b56b357289805952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d74fa738504d658b3f62e30bb4bcc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fb9e0e916a4eab8c0c68e3a43d2602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['document_id', 'sentences'],\n",
      "        num_rows: 10539\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['document_id', 'sentences'],\n",
      "        num_rows: 1370\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['document_id', 'sentences'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"conll2012_ontonotesv5\", \"english_v12\")\n",
    "\n",
    "#Checking\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f81abd5-bd1b-4ee0-bc47-e6f339b9043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document_id': Value(dtype='string', id=None), 'sentences': [{'part_id': Value(dtype='int32', id=None), 'words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'pos_tags': Sequence(feature=ClassLabel(names=['XX', '``', '$', \"''\", '*', ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'VERB', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None), 'parse_tree': Value(dtype='string', id=None), 'predicate_lemmas': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'predicate_framenet_ids': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'word_senses': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'speaker': Value(dtype='string', id=None), 'named_entities': Sequence(feature=ClassLabel(names=['O', 'B-PERSON', 'I-PERSON', 'B-NORP', 'I-NORP', 'B-FAC', 'I-FAC', 'B-ORG', 'I-ORG', 'B-GPE', 'I-GPE', 'B-LOC', 'I-LOC', 'B-PRODUCT', 'I-PRODUCT', 'B-DATE', 'I-DATE', 'B-TIME', 'I-TIME', 'B-PERCENT', 'I-PERCENT', 'B-MONEY', 'I-MONEY', 'B-QUANTITY', 'I-QUANTITY', 'B-ORDINAL', 'I-ORDINAL', 'B-CARDINAL', 'I-CARDINAL', 'B-EVENT', 'I-EVENT', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'B-LAW', 'I-LAW', 'B-LANGUAGE', 'I-LANGUAGE'], id=None), length=-1, id=None), 'srl_frames': [{'verb': Value(dtype='string', id=None), 'frames': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}], 'coref_spans': Sequence(feature=Sequence(feature=Value(dtype='int32', id=None), length=3, id=None), length=-1, id=None)}]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7cf41-f7ee-4fc5-af7d-722af631559d",
   "metadata": {},
   "source": [
    "## Processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9a18396-d68f-4208-85cb-08a7ba7556b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-PERSON', 'I-PERSON', 'B-NORP', 'I-NORP', 'B-FAC', 'I-FAC', 'B-ORG', 'I-ORG', 'B-GPE', 'I-GPE', 'B-LOC', 'I-LOC', 'B-PRODUCT', 'I-PRODUCT', 'B-DATE', 'I-DATE', 'B-TIME', 'I-TIME', 'B-PERCENT', 'I-PERCENT', 'B-MONEY', 'I-MONEY', 'B-QUANTITY', 'I-QUANTITY', 'B-ORDINAL', 'I-ORDINAL', 'B-CARDINAL', 'I-CARDINAL', 'B-EVENT', 'I-EVENT', 'B-WORK_OF_ART', 'I-WORK_OF_ART', 'B-LAW', 'I-LAW', 'B-LANGUAGE', 'I-LANGUAGE']\n"
     ]
    }
   ],
   "source": [
    "#Extracting all unique NER labels\n",
    "unique_labels = dataset[\"train\"].features[\"sentences\"][0][\"named_entities\"].feature.names\n",
    "\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6dd36579-eb05-4a6a-9d63-191c7c5d5047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5c0db-4305-4fae-8079-157a452afbea",
   "metadata": {},
   "source": [
    "## Converting data to tokens and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de551cca-1dcd-4252-bf61-08298b5f7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens_labels(dataset_split):\n",
    "    all_tokens, all_labels = [], []\n",
    "    for doc in dataset_split:\n",
    "        for sent in doc[\"sentences\"]:\n",
    "            tokens = sent[\"words\"]\n",
    "            label_ids = sent[\"named_entities\"]\n",
    "            all_tokens.append(tokens)\n",
    "            all_labels.append(label_ids)\n",
    "    return all_tokens, all_labels\n",
    "\n",
    "train_tokens, train_label_ids = extract_tokens_labels(dataset[\"train\"])\n",
    "val_tokens, val_label_ids = extract_tokens_labels(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee4375d-57f9-46b5-8c78-b9e7f3af97f0",
   "metadata": {},
   "source": [
    "## Preparing BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c342598-ede7-4db3-aa0c-a800ae9fbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb208de-f7ec-493d-a7c7-16599afa49d4",
   "metadata": {},
   "source": [
    "## Tokenizing and aligning labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0a4685d-b7ce-458f-8448-8ad6fbc04118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91562e78902e47abac170af3e0d89e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/115812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda79bf7a1b441b197b6cd130ace9660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(tokens_list, label_ids_list):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        tokens_list,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(label_ids_list):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(labels[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"tokens\": train_tokens, \"labels\": train_label_ids})\n",
    "val_dataset = Dataset.from_dict({\"tokens\": val_tokens, \"labels\": val_label_ids})\n",
    "\n",
    "train_tokenized = train_dataset.map(lambda x: tokenize_and_align_labels(x[\"tokens\"], x[\"labels\"]), batched=True)\n",
    "val_tokenized = val_dataset.map(lambda x: tokenize_and_align_labels(x[\"tokens\"], x[\"labels\"]), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2501afa-b528-444f-89d0-0472f65dbeaf",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfea0125-1668-409a-a70d-21acc8ac7834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff19baf-f7b6-4788-90f7-3c95b1336374",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d232f8bf-3375-4597-a806-24ff59c5bfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l1/dk38ftmj30j3r7gf2k6s2v_c0000gn/T/ipykernel_90170/822673382.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28954' max='28954' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28954/28954 1:30:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.074140</td>\n",
       "      <td>0.862763</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "    CARDINAL       0.77      0.88      0.82      1719\n",
       "        DATE       0.83      0.89      0.86      3197\n",
       "       EVENT       0.64      0.56      0.60       179\n",
       "         FAC       0.49      0.56      0.52       133\n",
       "         GPE       0.91      0.93      0.92      3618\n",
       "    LANGUAGE       0.87      0.74      0.80        35\n",
       "         LAW       0.49      0.70      0.58        64\n",
       "         LOC       0.68      0.77      0.72       316\n",
       "       MONEY       0.88      0.89      0.89       834\n",
       "        NORP       0.85      0.91      0.88      1277\n",
       "     ORDINAL       0.72      0.89      0.80       335\n",
       "         ORG       0.84      0.91      0.88      3787\n",
       "     PERCENT       0.90      0.90      0.90       656\n",
       "      PERSON       0.91      0.95      0.93      3144\n",
       "     PRODUCT       0.50      0.42      0.46       214\n",
       "    QUANTITY       0.72      0.69      0.70       190\n",
       "        TIME       0.68      0.78      0.72       361\n",
       " WORK_OF_ART       0.40      0.52      0.46       202\n",
       "\n",
       "   micro avg       0.84      0.89      0.86     20261\n",
       "   macro avg       0.73      0.77      0.75     20261\n",
       "weighted avg       0.84      0.89      0.86     20261\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.071645</td>\n",
       "      <td>0.874543</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "    CARDINAL       0.80      0.88      0.84      1719\n",
       "        DATE       0.85      0.89      0.87      3197\n",
       "       EVENT       0.67      0.54      0.60       179\n",
       "         FAC       0.45      0.63      0.53       133\n",
       "         GPE       0.92      0.94      0.93      3618\n",
       "    LANGUAGE       0.74      0.74      0.74        35\n",
       "         LAW       0.50      0.72      0.59        64\n",
       "         LOC       0.73      0.76      0.74       316\n",
       "       MONEY       0.89      0.91      0.90       834\n",
       "        NORP       0.87      0.90      0.88      1277\n",
       "     ORDINAL       0.75      0.83      0.79       335\n",
       "         ORG       0.87      0.90      0.89      3787\n",
       "     PERCENT       0.90      0.89      0.90       656\n",
       "      PERSON       0.91      0.96      0.93      3144\n",
       "     PRODUCT       0.55      0.46      0.50       214\n",
       "    QUANTITY       0.76      0.71      0.73       190\n",
       "        TIME       0.71      0.81      0.76       361\n",
       " WORK_OF_ART       0.50      0.57      0.54       202\n",
       "\n",
       "   micro avg       0.86      0.89      0.87     20261\n",
       "   macro avg       0.74      0.78      0.76     20261\n",
       "weighted avg       0.86      0.89      0.87     20261\n",
       "</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/envs/project_text_mining/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=28954, training_loss=0.06552902048296334, metrics={'train_runtime': 5416.4395, 'train_samples_per_second': 42.763, 'train_steps_per_second': 5.346, 'total_flos': 1.5135437830404096e+16, 'train_loss': 0.06552902048296334, 'epoch': 2.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-bert\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(-1)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return {\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "        \"report\": classification_report(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9db4b-688c-4942-ad53-9b1be0795d32",
   "metadata": {},
   "source": [
    "## Evaluating trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10819547-533c-45ef-af44-992b94fb236e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.07164479792118073,\n",
       " 'eval_f1': 0.8745434756318781,\n",
       " 'eval_report': '              precision    recall  f1-score   support\\n\\n    CARDINAL       0.80      0.88      0.84      1719\\n        DATE       0.85      0.89      0.87      3197\\n       EVENT       0.67      0.54      0.60       179\\n         FAC       0.45      0.63      0.53       133\\n         GPE       0.92      0.94      0.93      3618\\n    LANGUAGE       0.74      0.74      0.74        35\\n         LAW       0.50      0.72      0.59        64\\n         LOC       0.73      0.76      0.74       316\\n       MONEY       0.89      0.91      0.90       834\\n        NORP       0.87      0.90      0.88      1277\\n     ORDINAL       0.75      0.83      0.79       335\\n         ORG       0.87      0.90      0.89      3787\\n     PERCENT       0.90      0.89      0.90       656\\n      PERSON       0.91      0.96      0.93      3144\\n     PRODUCT       0.55      0.46      0.50       214\\n    QUANTITY       0.76      0.71      0.73       190\\n        TIME       0.71      0.81      0.76       361\\n WORK_OF_ART       0.50      0.57      0.54       202\\n\\n   micro avg       0.86      0.89      0.87     20261\\n   macro avg       0.74      0.78      0.76     20261\\nweighted avg       0.86      0.89      0.87     20261\\n',\n",
       " 'eval_runtime': 83.4713,\n",
       " 'eval_samples_per_second': 187.849,\n",
       " 'eval_steps_per_second': 23.481,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef1fa2-783b-4002-8300-1ed8bd7285d0",
   "metadata": {},
   "source": [
    "## Processing test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "886bda3f-f8b8-4e9d-9002-9001b6992ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing test file\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "tokens = []\n",
    "labels = []\n",
    "with open(\"NER-test.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    skip_header = True\n",
    "    for line in f:\n",
    "        if skip_header:\n",
    "            skip_header = False\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tokens:\n",
    "                test_sentences.append(tokens)\n",
    "                test_labels.append(labels)\n",
    "                tokens = []\n",
    "                labels = []\n",
    "        else:\n",
    "            parts = line.split('\\t')\n",
    "            tokens.append(parts[1])\n",
    "            labels.append(parts[2])\n",
    "    if tokens:\n",
    "        test_sentences.append(tokens)\n",
    "        test_labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a2f8e78-367d-4f61-9c08-ad9d8027d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the 'B-LOCATION'/'I-LOCATION' labels\n",
    "\n",
    "def normalize_location_label(label):\n",
    "    if label == \"B-LOCATION\":\n",
    "        return \"B-LOC\"\n",
    "    elif label == \"I-LOCATION\":\n",
    "        return \"I-LOC\"\n",
    "    else:\n",
    "        return label\n",
    "\n",
    "test_labels_normalized = [\n",
    "    [normalize_location_label(tag) for tag in sent_labels] for sent_labels in test_labels\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b37a5e8c-3350-4b4c-ba8c-2f64898997d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping\n",
    "test_label_ids = [\n",
    "    [label2id.get(label, label2id['O']) for label in sent_labels]\n",
    "    for sent_labels in test_labels_normalized\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c29aca-76c1-41fd-9928-8c162b575776",
   "metadata": {},
   "source": [
    "## Tokenizing and aligning test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f970974-ea14-4945-86f0-f165e46188d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad420de3215495b953ae272a72b5545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = Dataset.from_dict({\"tokens\": test_sentences, \"labels\": test_label_ids})\n",
    "\n",
    "def tokenize_and_align_labels(tokens_list, label_ids_list):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        tokens_list,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(label_ids_list):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(labels[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "test_tokenized = test_dataset.map(lambda x: tokenize_and_align_labels(x[\"tokens\"], x[\"labels\"]), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28db8b-ec4a-445a-beba-bc37b5f009cc",
   "metadata": {},
   "source": [
    "## Predicting on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ce53a4c-4239-470f-b337-e9016fc0a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized_for_pred = test_tokenized.remove_columns('labels')\n",
    "test_results = trainer.predict(test_tokenized_for_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188eeefd-23f4-4c85-b1be-3ba23bfd8048",
   "metadata": {},
   "source": [
    "## Converting IDs back to strings for reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b7b20-fc4f-452d-bcef-691eb9f4fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_labels = [\n",
    "    [id2label[p] for (p, l) in zip(pred_seq, label_seq) if l != -100]\n",
    "    for pred_seq, label_seq in zip(preds, test_tokenized[\"labels\"])\n",
    "]\n",
    "all_true_labels = [\n",
    "    [id2label[l] for (p, l) in zip(pred_seq, label_seq) if l != -100]\n",
    "    for pred_seq, label_seq in zip(preds, test_tokenized[\"labels\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8419daa0-95d9-4998-99a6-28a259761c17",
   "metadata": {},
   "source": [
    "## Evaluation metrics of test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d93fc-c003-437c-99a5-fe0e151d358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "print(classification_report(all_true_labels, all_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c4d35-752a-4e72-9b80-f19b077e688f",
   "metadata": {},
   "source": [
    "## Bar graph of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bb294d-8b3e-4054-b3ef-e0db2455acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting per-class scores using seqeval\n",
    "\n",
    "from seqeval.metrics import classification_report, performance_report\n",
    "import pandas as pd\n",
    "\n",
    "#Returns a report as a dictionary\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "report_dict = classification_report(all_true_labels, all_pred_labels, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7c4fe-e2c2-4dc5-99c2-2d3437c1f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe for table\n",
    "\n",
    "labels = [label for label in report_dict if label not in ['micro avg', 'macro avg', 'weighted avg', 'accuracy']]\n",
    "\n",
    "df_report = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Precision\": [report_dict[l][\"precision\"] for l in labels],\n",
    "    \"Recall\":    [report_dict[l][\"recall\"]    for l in labels],\n",
    "    \"F1-score\":  [report_dict[l][\"f1-score\"]  for l in labels],\n",
    "    \"Support\":   [report_dict[l][\"support\"]   for l in labels]\n",
    "})\n",
    "\n",
    "df_report = df_report.sort_values(\"F1-score\", ascending=False)\n",
    "\n",
    "#Displaying\n",
    "from IPython.display import display\n",
    "display(df_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fbb06f8-e604-4ced-975c-bac29ffe9688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar Graph of Per-Class F1 Scores\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df_report[\"Label\"], df_report[\"F1-score\"])\n",
    "plt.xlabel(\"Entity Label\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Per-class F1 Scores (NER)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab6083-24ae-44fd-8837-9ec0c6a8056c",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "953d657e-ed4e-4012-bb76-9c2a594e6726",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_true_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m flat_true = []\n\u001b[32m      2\u001b[39m flat_pred = []\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m true_seq, pred_seq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_true_labels, all_pred_labels):\n\u001b[32m      5\u001b[39m     flat_true.extend(true_seq)\n\u001b[32m      6\u001b[39m     flat_pred.extend(pred_seq)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_true_labels' is not defined"
     ]
    }
   ],
   "source": [
    "#Flattening predictions\n",
    "flat_true = []\n",
    "flat_pred = []\n",
    "\n",
    "for true_seq, pred_seq in zip(all_true_labels, all_pred_labels):\n",
    "    flat_true.extend(true_seq)\n",
    "    flat_pred.extend(pred_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b23a7-2107-4d74-ad8e-43def08457ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Optional: If you want to exclude \"O\" tags, filter here\n",
    "labels_to_include = sorted(list(set(flat_true + flat_pred)))  # All seen labels\n",
    "\n",
    "cm = confusion_matrix(flat_true, flat_pred, labels=labels_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69038d52-f614-449c-bd0f-241f6afb133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_to_include)\n",
    "disp.plot(include_values=True, cmap=\"Blues\", ax=ax, xticks_rotation=45)\n",
    "plt.title(\"NER Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
